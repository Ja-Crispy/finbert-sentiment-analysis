{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import wandb\n",
    "from multiprocessing import Pool, cpu_count, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(chunk_idx, headlines_list, stocks_list, result_list):\n",
    "    print(f\"Processing chunk {chunk_idx}\")\n",
    "\n",
    "    # Initialize model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "    # Disable gradient calculations for faster inference\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input data\n",
    "    headlines_chunk = headlines_list[chunk_idx]\n",
    "    stocks_chunk = stocks_list[chunk_idx]\n",
    "    inputs = tokenizer(headlines_chunk, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Save the prediction results to the shared result list\n",
    "    results = []\n",
    "    for headline, stock, pos, neg, neutr in zip(headlines_chunk, stocks_chunk,\n",
    "                                                prediction[:, 0].tolist(),\n",
    "                                                prediction[:, 1].tolist(),\n",
    "                                                prediction[:, 2].tolist()):\n",
    "        results.append((headline, stock, pos, neg, neutr))\n",
    "\n",
    "    result_list[chunk_idx] = results\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # Load data from CSV file\n",
    "    headlines_df = pd.read_csv('D:/AIML projects/Financial Sentiment Analysis/5000_preprocessed_analyst_ratings.csv')\n",
    "    headlines_array = np.array(headlines_df)\n",
    "    headlines_list = list(headlines_array[:, 2])\n",
    "    stocks_list = list(headlines_array[:, -1])\n",
    "    return headlines_list, stocks_list\n",
    "\n",
    "\n",
    "def chunk_data(data_list, chunk_size):\n",
    "    # Chunk the input data into smaller pieces\n",
    "    return [data_list[i:i + chunk_size] for i in range(0, len(data_list), chunk_size)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load input data\n",
    "    headlines_list, stocks_list = load_data()\n",
    "\n",
    "    # Split input data into chunks\n",
    "    num_chunks = cpu_count()\n",
    "    chunk_size = len(headlines_list) // num_chunks\n",
    "    headlines_list_chunks = chunk_data(headlines_list, chunk_size)\n",
    "    stocks_list_chunks = chunk_data(stocks_list, chunk_size)\n",
    "\n",
    "    # Initialize shared result list using multiprocessing.Manager()\n",
    "    manager = Manager()\n",
    "    result_list = manager.list([[] for _ in range(num_chunks)])\n",
    "\n",
    "    # Initialize a pool of worker processes\n",
    "    with Pool(processes=num_chunks) as pool:\n",
    "        # Map each worker process to a chunk of input data\n",
    "        for idx in range(num_chunks):\n",
    "            pool.apply_async(predict, args=(idx, headlines_list_chunks, stocks_list_chunks, result_list))\n",
    "\n",
    "        # Wait for all worker processes to complete\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    # Consolidate results from the shared result list\n",
    "    headlines_table = wandb.Table(columns=[\"Headline\", \"Stock\", \"Positive\", \"Negative\", \"Neutral\"])\n",
    "    for chunk_results in result_list:\n",
    "        for headline, stock, pos, neg, neutr in chunk_results:\n",
    "            headlines_table.add_data(headline, stock, pos, neg, neutr)\n",
    "\n",
    "    # Log the consolidated results to wandb\n",
    "    wandb.init(project=\"Financial_Sentiment_Analysis\")\n",
    "    wandb.run.log({\"Financial Sentiment Analysis Table\": headlines_table})\n",
    "    wandb.run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
